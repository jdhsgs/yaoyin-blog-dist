(window.webpackJsonp=window.webpackJsonp||[]).push([[13],{549:function(t,a,s){"use strict";s.r(a);var r=s(12),e=Object(r.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"embedding"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#embedding"}},[t._v("#")]),t._v(" Embedding")]),t._v(" "),s("h2",{attrs:{id:"word2vec"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#word2vec"}},[t._v("#")]),t._v(" Word2Vec")]),t._v(" "),s("p",[t._v("Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，它被大量地用在自然语言处理（NLP）中。那么它是如何帮助我们做自然语言处理呢？Word2Vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。Embedding其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。")]),t._v(" "),s("p",[t._v("基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。基于训练数据建模的过程，我们给它一个名字叫“Fake Task”，意味着建模并不是我们最终的目的")]),t._v(" "),s("p",[t._v("实际需要的不是输出层，而是隐层")]),t._v(" "),s("p",[t._v("上面提到的这种方法实际上会在无监督特征学习（unsupervised feature learning）中见到，最常见的就是自编码器（auto-encoder）：通过在隐层将输入进行编码压缩，继而在输出层将数据解码恢复初始状态，训练完成后，我们会将输出层“砍掉”，仅保留隐层")]),t._v(" "),s("h3",{attrs:{id:"the-fake-task"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#the-fake-task"}},[t._v("#")]),t._v(" "),s("strong",[t._v("The Fake Task")])]),t._v(" "),s("p",[t._v("训练模型的真正目的是获得模型基于训练数据学得的隐层权重。")]),t._v(" "),s("p",[t._v("首先要构建一个完整的神经网络作为我们的“Fake Task”，后面再返回来看通过“Fake Task”我们如何间接地得到这些词向量。")]),t._v(" "),s("p",[t._v("如何训练我们的神经网络。假如我们有一个句子**“The dog barked at the mailman”。**")]),t._v(" "),s("p",[t._v("首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；")]),t._v(" "),s("p",[t._v("再定义一个叫做skip_window的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设置"),s("img",{attrs:{src:"https://www.zhihu.com/equation?tex=skip%5C_window%3D2",alt:"[公式]"}}),t._v("，那么我们最终获得窗口中的词（包括input word在内)就是**['The', 'dog'，'barked', 'at']**。")]),t._v(" "),s("p",[t._v("另一个参数叫num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word")]),t._v(" "),s("p",[t._v("神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词是output word的可能性")]),t._v(" "),s("p",[t._v("模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。举个栗子，如果我们向神经网络模型中输入一个单词“Soviet“，那么最终模型的输出概率中，像“Union”， ”Russia“这种相关词的概率将远高于像”watermelon“，”kangaroo“非相关词的概率。因为”Union“，”Russia“在文本中更大可能在”Soviet“的窗口中出现。")]),t._v(" "),s("p",[t._v("我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。下面的图中给出了一些我们的训练样本的例子。我们选定句子**“The quick brown fox jumps over lazy dog”**，设定我们的窗口大小为2（"),s("img",{attrs:{src:"https://www.zhihu.com/equation?tex=window%5C_size%3D2",alt:"[公式]"}}),t._v(")，也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://pic4.zhimg.com/80/v2-ca21f9b1923e201c4349030a86f6dc1f_1440w.png",alt:"v2-ca21f9b1923e201c4349030a86f6dc1f_1440w"}})]),t._v(" "),s("p",[t._v("我们的模型将会从每对单词出现的次数中习得统计结果。例如，我们的神经网络可能会得到更多类似（“Soviet“，”Union“）这样的训练样本对，而对于（”Soviet“，”Sasquatch“）这样的组合却看到的很少。因此，当我们的模型完成训练后，给定一个单词”Soviet“作为输入，输出的结果中”Union“或者”Russia“要比”Sasquatch“被赋予更高的概率。")]),t._v(" "),s("h3",{attrs:{id:"模型细节"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#模型细节"}},[t._v("#")]),t._v(" "),s("strong",[t._v("模型细节")])]),t._v(" "),s("p",[t._v("首先，我们都知道神经网络只能接受数值输入，我们不可能把一个单词字符串作为输入，因此我们得想个办法来表示这些单词。最常用的办法就是基于训练文档来构建我们自己的词汇表（vocabulary）再对单词进行one-hot编码。")]),t._v(" "),s("p",[t._v("假设从我们的训练文档中抽取出10000个唯一不重复的单词组成词汇表。我们对这10000个单词进行one-hot编码，得到的每个单词都是一个10000维的向量，向量每个维度的值只有0或者1，假如单词ants在词汇表中的出现位置为第3个，那么ants的向量就是一个第三维度取值为1，其他维都为0的10000维的向量（"),s("img",{attrs:{src:"https://www.zhihu.com/equation?tex=ants%3D%5B0%2C+0%2C+1%2C+0%2C+...%2C+0%5D",alt:"[公式]"}}),t._v(")")]),t._v(" "),s("p",[t._v('还是上面的例子，“The dog barked at the mailman”，那么我们基于这个句子，可以构建一个大小为5的词汇表（忽略大小写和标点符号）：("the", "dog", "barked", "at", "mailman")，我们对这个词汇表的单词进行编号0-4。那么”dog“就可以被表示为一个5维向量[0, 1, 0, 0, 0]。')]),t._v(" "),s("p",[t._v("模型的输入如果为一个10000维的向量，那么输出也是一个10000维度（词汇表的大小）的向量，它包含了10000个概率，每一个概率代表着当前词是输入样本中output word的概率大小。")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://pic2.zhimg.com/80/v2-5c16aa8eaa670485ed5cbcd68e4b8b41_1440w.png",alt:"v2-5c16aa8eaa670485ed5cbcd68e4b8b41_1440w"}}),t._v("隐层没有使用任何激活函数，但是输出层使用了sotfmax。")]),t._v(" "),s("p",[t._v("如果我们现在想用300个特征来表示一个单词（即每个词可以被表示为300维的向量）。那么隐层的权重矩阵应该为10000行，300列（隐层有300个结点）。")]),t._v(" "),s("p",[t._v("看下面的图片，左右两张图分别从不同角度代表了输入层-隐层的权重矩阵。左图中每一列代表一个10000维的词向量和隐层单个神经元连接的权重向量。"),s("strong",[t._v("从右边的图来看，每一行实际上代表了每个单词的词向量。")])]),t._v(" "),s("p",[s("img",{attrs:{src:"https://pic1.zhimg.com/80/v2-c538566f7d627ce7ca40589f15ca8284_1440w.png",alt:"v2-c538566f7d627ce7ca40589f15ca8284_1440w"}})]),t._v(" "),s("p",[t._v("所以我们最终的目标就是学习这个隐层的权重矩阵。")]),t._v(" "),s("h3",{attrs:{id:"输出层"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#输出层"}},[t._v("#")]),t._v(" "),s("strong",[t._v("输出层")])]),t._v(" "),s("p",[t._v("经过神经网络隐层的计算，ants这个词会从一个1 x 10000的向量变成1 x 300的向量，再被输入到输出层。输出层是一个softmax回归分类器，它的每个结点将会输出一个0-1之间的值（概率），这些所有输出层神经元结点的概率之和为1。")]),t._v(" "),s("h3",{attrs:{id:"直觉上的理解"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#直觉上的理解"}},[t._v("#")]),t._v(" "),s("strong",[t._v("直觉上的理解")])]),t._v(" "),s("p",[t._v("如果两个不同的单词有着非常相似的“上下文”（也就是窗口单词很相似，比如“Kitty climbed the tree”和“Cat climbed the tree”），那么通过我们的模型训练，这两个单词的嵌入向量将非常相似。")]),t._v(" "),s("p",[t._v("那么两个单词拥有相似的“上下文”到底是什么含义呢？比如对于同义词“intelligent”和“smart”，我们觉得这两个单词应该拥有相同的“上下文”。而例如”engine“和”transmission“这样相关的词语，可能也拥有着相似的上下文。")]),t._v(" "),s("p",[t._v("实际上，这种方法实际上也可以帮助你进行词干化（stemming），例如，神经网络对”ant“和”ants”两个单词会习得相似的词向量。")]),t._v(" "),s("h3",{attrs:{id:"创新"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#创新"}},[t._v("#")]),t._v(" 创新")]),t._v(" "),s("ol",[s("li",[t._v("将常见的单词组合（word pairs）或者词组作为单个“words”来处理。")]),t._v(" "),s("li",[t._v("对高频次单词进行抽样来减少训练样本的个数。")]),t._v(" "),s("li",[t._v("对优化目标采用“negative sampling”方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担。")])]),t._v(" "),s("h3",{attrs:{id:"skip-gram"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#skip-gram"}},[t._v("#")]),t._v(" skip-gram")]),t._v(" "),s("p",[t._v("用一个词语作为输入，来预测它周围的上下文")]),t._v(" "),s("h3",{attrs:{id:"cbow"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#cbow"}},[t._v("#")]),t._v(" CBOW")]),t._v(" "),s("p",[t._v("拿一个词语的上下文作为输入，来预测这个词语本身")]),t._v(" "),s("h3",{attrs:{id:"skip-gram-和-cbow-的简单情形"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#skip-gram-和-cbow-的简单情形"}},[t._v("#")]),t._v(" "),s("strong",[t._v("Skip-gram 和 CBOW 的简单情形")]),t._v("：")]),t._v(" "),s("p",[t._v("当上下文只有一个词时，语言模型就简化为：用当前词 x 预测它的下一个词 y")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://img2018.cnblogs.com/blog/646262/201905/646262-20190510144351983-196404156.png",alt:"646262-20190510144351983-196404156"}})]),t._v(" "),s("h3",{attrs:{id:"skip-gram更一般的情形"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#skip-gram更一般的情形"}},[t._v("#")]),t._v(" "),s("strong",[t._v("Skip-gram更一般的情形：")])]),t._v(" "),s("p",[s("img",{attrs:{src:"https://pic1.zhimg.com/80/v2-576f07f2465997a9c2b2e2577c3aed1c_1440w.jpg",alt:"v2-576f07f2465997a9c2b2e2577c3aed1c_1440w"}})]),t._v(" "),s("h3",{attrs:{id:"cbow更一般的情形"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#cbow更一般的情形"}},[t._v("#")]),t._v(" "),s("strong",[t._v("CBOW更一般的情形")]),t._v("：")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://pic3.zhimg.com/v2-2d075f9cf9527553f401879c65d39ebe_r.jpg",alt:"v2-2d075f9cf9527553f401879c65d39ebe_r"}})])])}),[],!1,null,null,null);a.default=e.exports}}]);